{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b42780",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a0eef",
   "metadata": {},
   "source": [
    "1.1.Image Formation. \n",
    "   \n",
    "1.1.1.geometry(äº†è§£). \n",
    "    Pinhole camera. \n",
    "    Projective geometry: perspective/affine. \n",
    "    Image distortions. \n",
    "\n",
    "1.1.2.Color module(æè¿°). \n",
    "    RGB:defaule,ç¼ºç‚¹:correalted channels. \n",
    "    HSV:Intuitive,ç”¨åœ¨color selection/manipulation,ç¼ºç‚¹: confound channels. \n",
    "    YCbCr: compression/compute,ç”¨åœ¨jpeg. \n",
    "    L*a*b*: Perceptually uniform,ç”¨åœ¨ps. \n",
    "\n",
    "1.1.3.Digitisation:by spatial sampling. \n",
    "    Spatial resolutin: åˆ†è¾¨ç‡ä¾èµ–åƒç´ . \n",
    "    Quantisation(intensity): 2^8^. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031216ae",
   "metadata": {},
   "source": [
    "2.Image Processing Basic\n",
    "   \n",
    "2.1.Spatial domain operationç©ºé—´åŸŸè¿ç®—. \n",
    "\n",
    "2.1.1.Point operationsç‚¹è¿ç®—,(æè¿°åŸç†,ç†è§£å¼ºåº¦ç›´æ–¹å›¾,å®šä¹‰è¿ç®—).\n",
    "\n",
    "2.1.1.1.åŸç†\n",
    "    Contrast stretchingå¯¹æ¯”æ‹‰ä¼¸:  \n",
    "    Intensity thresholdingå¼ºåº¦é˜ˆå€¼:  \n",
    "        Automatic thresholding:  \n",
    "            Otsu. \n",
    "            Isodata. \n",
    "    Multilevel thresholdingå¤šçº§é˜ˆå€¼:\n",
    "    Intensity Inversionå¼ºåº¦åæ¼”:\n",
    "    Log transformation:\n",
    "    Power transformation:\n",
    "    Piecewise Linear transformoationåˆ†æ®µçº¿æ€§å˜æ¢:\n",
    "    Piecewise contrast stretchingåˆ†æ®µå¯¹æ¯”æ‹‰ä¼¸:\n",
    "    Gray-level slicingç°åº¦åˆ‡ç‰‡:\n",
    "    Bit-plane slicingä½å¹³é¢åˆ‡ç‰‡:\n",
    "\n",
    "2.1.1.2.ç›´æ–¹å›¾. \n",
    "    Histogram processingç›´æ–¹å›¾å¤„ç†:\n",
    "        Histogram equalizationç›´æ–¹å›¾å‡è¡¡. \n",
    "            Constrainedå—çº¦æŸçš„:continuous/discrete. \n",
    "        Histogram specificaion(matching)ç›´æ–¹å›¾åŒ¹:continuous/discrete. \n",
    "    Arithmetic and logical operationsç®—æ•°é€»è¾‘è¿ç®—:\n",
    "        åŠ å‡,AND/OR\n",
    "    Averagingå¹³å‡åŒ–:reduce noise in images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee5b7d",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeca0ff",
   "metadata": {},
   "source": [
    "2.1.2.Neighbourhood operations (spatial filteringç©ºé—´æ»¤æ³¢ on groups of pixels)é‚»åŸŸå˜æ¢:\n",
    "\n",
    "2.1.2.1.Spatial filtering\n",
    "\n",
    "2.1.2.1.1.åŸç†\n",
    "    ä½¿ç”¨è¾“å…¥å›¾åƒä¸­ä¸€ä¸ªåƒç´ çš„å°é‚»åŸŸå†…çš„ç°åº¦å€¼æ¥ç”Ÿæˆè¾“å‡ºå›¾åƒä¸­è¯¥åƒç´ çš„æ–°ç°åº¦å€¼\n",
    "    æ ¹æ®å¯¹åƒç´ å€¼åº”ç”¨çš„æƒé‡ï¼Œå®ƒä»¬å¯ä»¥æŠ‘åˆ¶ï¼ˆæ»¤é™¤ï¼‰æˆ–å¢å¼ºä¿¡æ¯\n",
    "    æƒé‡çŸ©é˜µç§°ä¸ºæ»¤æ³¢å™¨filteræˆ–æ ¸kernel\n",
    "\n",
    "2.1.2.1.2.convolutionå·ç§¯å®ç°\n",
    "    è¾“å…¥å›¾åƒğ‘“(ğ‘¥, ğ‘¦)å’Œå·ç§¯æ ¸â„(ğ‘¥, ğ‘¦)è¿›è¡Œç¦»æ•£å·ç§¯discrete convolutionè®¡ç®—\n",
    "    the kernel must be flippedæ ¸å¿…é¡»ç¿»è½¬\n",
    "    Fixing the border problemè§£å†³è¾¹æ¡†é—®é¢˜\n",
    "        Paddingå¡«å……ï¼šå°†æ‰€æœ‰é™„åŠ åƒç´ è®¾ç½®ä¸ºå¸¸é‡ï¼ˆé›¶ï¼‰å€¼ï¼šç¡¬è¿‡æ¸¡ä¼šäº§ç”Ÿè¾¹æ¡†ä¼ªå½±ï¼ˆéœ€è¦çª—å£å¤„ç†ï¼‰\n",
    "        Clampingå¤¹æŒï¼šæ— é™é‡å¤æ‰€æœ‰è¾¹ç•Œåƒç´ å€¼ï¼šè¾¹ç•Œè¡Œä¸ºæ›´å¥½ä½†ä»»æ„ï¼ˆæ— ç†è®ºåŸºç¡€ï¼‰\n",
    "        WrappingåŒ…è£…ï¼šä»å¯¹ä¾§å¤åˆ¶åƒç´ å€¼ï¼šéšå¼ç”¨äºï¼ˆå¿«é€Ÿï¼‰å‚…é‡Œå¶å˜æ¢\n",
    "        Mirroringé•œåƒï¼šå°†åƒç´ å€¼æ²¿è¾¹ç•Œåå°„ï¼šå¹³æ»‘ã€å¯¹ç§°ã€å‘¨æœŸæ€§ã€æ— è¾¹ç•Œä¼ªå½±\n",
    "    linear, shift-invariant operationçº¿æ€§ã€å¹³ç§»ä¸å˜çš„è¿ç®—\n",
    "\n",
    "2.1.2.1.3.smoothing filterå¹³æ»‘æ»¤æ³¢å™¨(uniform filterå‡åŒ€æ»¤æ³¢å™¨)\n",
    "    Calculates mean pixel valueå¹³å‡åƒç´ å€¼ in a neighbourhood\n",
    "    used for image blurring and noise reductionå›¾åƒæ¨¡ç³Šå’Œé™å™ª\n",
    "    Reduces fluctuations due to disturbances in image acquisition\n",
    "    Neighbourhood averaging also blurs the object edgesç‰©ä½“è¾¹ç¼˜æ¨¡ç³Š in the image\n",
    "    Can use weighted averagingåŠ æƒå¹³å‡ to give more importance to some pixels\n",
    "\n",
    "2.1.2.1.4.Gaussian filteré«˜æ–¯æ»¤æ³¢å™¨:å¹³æ»‘smoothing\n",
    "    the only filter that is both separable and circularly symmetric\n",
    "    optimal joint localization in spatial and frequency domain\n",
    "    Fourier transform of a Gaussian is also a Gaussian function\n",
    "    n-fold convolution of any low-pass filter converges to a Gaussian\n",
    "    infinitely smooth so it can be differentiated to any desired degree\n",
    "    scales naturally (sigma) and allows for consistent scale-space theory\n",
    "    å¦‚æœå¿…é¡»ä¿ç•™å°ç‰©ä½“ï¼Œé«˜æ–¯æ»¤æ³¢æ•ˆæœæœ€ä½³\n",
    "\n",
    "2.1.2.1.5.Median filterä¸­å€¼æ»¤æ³¢å™¨(nonlinear filteréçº¿æ€§æ»¤æ³¢å™¨)\n",
    "    order-statistics filteråŸºäºæ’åºæ»¤æ³¢å™¨ (based on ordering and ranking pixel values)\n",
    "    Calculates the median pixel valueä¸­å€¼åƒç´ å€¼ in a neighbourhood ğ‘ with |ğ‘| pixels\n",
    "    The median value ğ‘š of a set of ordered values is the middle valueä¸­é—´çš„å€¼\n",
    "    most half the values in the set are < ğ‘š and the other half > ğ‘š\n",
    "    å–æœ€å°/æœ€å¤§å€¼å°±æ˜¯æœ€å°/æœ€å¤§æ»¤æ³¢\n",
    "    Forces pixels with distinct intensities to be more like their neighboursä½¿å…¶æ›´æ¥è¿‘å…¶é‚»å±…\n",
    "    It eliminates isolated intensity spikes (salt and pepper image noise)æ¶ˆé™¤å­¤ç«‹çš„å¼ºåº¦å°–å³°ï¼ˆæ¤’ç›å›¾åƒå™ªå£°ï¼‰\n",
    "    Neighbourhood is typically of size ğ’ Ã— ğ’ pixels é€šå¸¸æ˜¯ğ’ Ã— ğ’åƒç´  with ğ‘› = 3, 5, 7, \n",
    "    eliminates pixel clustersæ¶ˆé™¤äº†åƒç´ ç°‡ (light or dark) with area < ğ‘› 2/2\n",
    "    å¦‚æœå¿…é¡»å»é™¤å°ç‰©ä½“ï¼Œä¸­å€¼æ»¤æ³¢æ•ˆæœæœ€ä½³\n",
    "\n",
    "2.1.2.1.6.Poolingæ± åŒ–\n",
    "    Combines filtering and downsampling in one operation\n",
    "    Examples include max / min / median / average pooling\n",
    "    Makes the image smaller and reduces computations\n",
    "    Popular in deep convolutional neural networks\n",
    "\n",
    "2.1.2.1.7.Derivative filterså¯¼æ•°æ»¤æ³¢å™¨\n",
    "    Spatial derivatives respond to intensity changes (such as object edges)\n",
    "    In digital images they are approximated using finite differences\n",
    "    Different possible ways to take finite differences\n",
    "    Gaussian derivative filtersé«˜æ–¯å¯¼æ•°æ»¤æ³¢å™¨\n",
    "        Extension of Gaussian filter kernels to 2D and different spatial scales\n",
    "\n",
    "2.1.2.1.8.Prewitt and Sobel kernelsæ™®é²ä¼Šç‰¹å’Œç´¢è´å°”æ ¸\n",
    "    Differentiation in one dimension and smoothing in the other dimension\n",
    "\n",
    "2.1.2.1.9.Separable filter kernelså¯åˆ†ç¦»æ»¤æ³¢å™¨æ ¸\n",
    "    Allow for a much more computationally efficient implementation\n",
    "\n",
    "2.1.2.1.10.Laplacean filteringæ‹‰æ™®æ‹‰æ–¯æ»¤æ³¢\n",
    "    Approximating the sum of second-order derivatives\n",
    "    Sharpening using the Laplaceanä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­é”åŒ–\n",
    "\n",
    "2.1.2.1.11.Intensity gradient vectorå¼ºåº¦æ¢¯åº¦å‘é‡\n",
    "    Gradient vector(2D)æ¢¯åº¦å‘é‡\n",
    "        Points in the direction of steepest intensity increase\n",
    "        Is orthogonal to isophotes (lines of equal intensity)\n",
    "    Gradient magnitude(2D)æ¢¯åº¦å¹…åº¦\n",
    "        Represents the length of the gradient vector\n",
    "        Is the magnitude of the local intensity change\n",
    "        Edge detectionè¾¹ç¼˜æ£€æµ‹:\n",
    "            Edge detection with the Laplacean\n",
    "\n",
    "2.1.2.1.12.Selecting the right spatial scaleé€‰æ‹©åˆé€‚çš„ç©ºé—´å°ºåº¦\n",
    "    Computing image derivatives using Gaussian derivative kernels\n",
    "\n",
    "2.1.2.1.13.Differentiation in the Fourier domainå‚…é‡Œå¶åŸŸä¸­çš„å¾®åˆ†\n",
    "    Differentiation suppresses low frequencies but blows up high frequencies (including noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a87258",
   "metadata": {},
   "source": [
    "2.2.Transform domain operations (mainly in Fourier space)å˜æ¢åŸŸæ“ä½œï¼ˆä¸»è¦åœ¨å‚…é‡Œå¶ç©ºé—´ä¸­ï¼‰\n",
    "\n",
    "2.2.1.Frequency domain\n",
    "    High frequencies correspond to 1.rapidly changing intensities across pixel é«˜é¢‘å¯¹åº”äºåƒç´ é—´å¿«é€Ÿå˜åŒ–çš„å¼ºåº¦\n",
    "    Low frequency components correspond to 1.large-scale image structures ä½é¢‘åˆ†é‡å¯¹åº”äºå›¾åƒçš„å¤§è§„æ¨¡ç»“æ„\n",
    "                                           2.slowly varying intensities across pixels åƒç´ é—´ç¼“æ…¢å˜åŒ–çš„å¼ºåº¦\n",
    "    Noise typically corresponds to fluctuations in the highest frequencies å™ªå£°é€šå¸¸å¯¹åº”äºæœ€é«˜é¢‘ç‡çš„æ³¢åŠ¨\n",
    "    Frequency domain image processing via the Fourier transform é€šè¿‡å‚…é‡Œå¶å˜æ¢è¿›è¡Œé¢‘åŸŸå›¾åƒå¤„ç†\n",
    "        Fourier images are typically centred for visualisation and processing å‚…é‡Œå¶å›¾åƒé€šå¸¸ç”¨äºå¯è§†åŒ–å’Œå¤„ç†æ—¶ä¼šè¿›è¡Œä¸­å¿ƒåŒ–\n",
    "\n",
    "\n",
    "2.2.2.Fourier transform (1D)/(2D)\n",
    "    Forward:\n",
    "    Inverse:\n",
    "        Uses complex valued sinusoids\n",
    "\n",
    "2.2.3.Discrete Fourier transform (DFT)ç¦»æ•£å‚…é‡Œå¶å˜æ¢\n",
    "    Digital images are discrete 2D functions\n",
    "    The discrete Fourier transform and its inverse always exist\n",
    "    Forward:\n",
    "    Inverse:\n",
    "\n",
    "2.2.4.real-valued functionså®å€¼å‡½æ•°çš„å‚…é‡Œå¶å˜æ¢\n",
    "    A real-valued function always has a conjugate symmetric Fourier transform\n",
    "\n",
    "2.2.5.Procedure for frequency domain filtering\n",
    "    1. Multiply the input image ğ‘“(ğ‘¥, ğ‘¦) by (âˆ’1)^(x + ğ‘¦) to ensure centering ğ¹(ğ‘¢, ğ‘£)\n",
    "    2. Compute the transform ğ¹(ğ‘¢, ğ‘£) from image ğ‘“(ğ‘¥, ğ‘¦) using the 2D DFT\n",
    "    3. Multiply ğ¹(ğ‘¢, ğ‘£) by a centred filter ğ»(ğ‘¢, ğ‘£) to obtain the result ğº(ğ‘¢, ğ‘£)\n",
    "    4. Compute the inverse 2D DFT of ğº(ğ‘¢, ğ‘£) to obtain the spatial result ğ‘”(ğ‘¥, ğ‘¦)\n",
    "    5. Take the real component of ğ‘”(ğ‘¥, ğ‘¦) (the imaginary component is zero)\n",
    "    6. Multiply the result by (âˆ’1)^(x + ğ‘¦) to remove the pattern introduced in step 1\n",
    "\n",
    "2.2.6.low-pass filteringä½é€šæ»¤æ³¢(blurred version of the original)\n",
    "\n",
    "2.2.7.notch filteringé™·æ³¢æ»¤æ³¢\n",
    "\n",
    "2.2.8.Exploiting the convolution theoremåˆ©ç”¨å·ç§¯å®šç†\n",
    "    é¢‘åŸŸä¸­çš„æ»¤æ³¢åœ¨è®¡ç®—ä¸Šå¯èƒ½æ›´é«˜æ•ˆ\n",
    "    åœ¨é¢‘åŸŸä¸­è®¾è®¡æ»¤æ³¢å™¨å¯èƒ½æ›´ç›´è§‚\n",
    "    ä½é€šæ»¤æ³¢å™¨ï¼šä¿ç•™ä½é¢‘ä½†è¡°å‡é«˜é¢‘\n",
    "    é«˜é€šæ»¤æ³¢å™¨ï¼šä¿ç•™é«˜é¢‘ä½†è¡°å‡ä½é¢‘\n",
    "    å¸¦é€šæ»¤æ³¢å™¨ï¼šä¿ç•™ç»™å®šé¢‘æ®µå†…çš„é¢‘ç‡å¹¶è¡°å‡å…¶ä½™é¢‘ç‡\n",
    "    è¿›è¡Œé€†å˜æ¢ä»¥è·å¾—ç›¸åº”çš„ç©ºé—´æ»¤æ³¢å™¨\n",
    "\n",
    "2.2.9.Gaussian filter\n",
    "    low-pass:\n",
    "    high-pass:Approximation of an inverted Laplacean filter\n",
    "\n",
    "2.2.10.Multiresolution image processing å¤šåˆ†è¾¨ç‡å›¾åƒå¤„ç†\n",
    "    Small objects and fine details benefit from high resolution\n",
    "    Large objects and coarse structures can make do with lower resolution\n",
    "    If both are present at the same time, multiple resolutions may be useful\n",
    "    This requires computing image pyramids\n",
    "\n",
    "2.2.11.Creating image pyramids åˆ›å»ºå›¾åƒé‡‘å­—å¡”\n",
    "    1.Compute an approximation of the input image by filtering and downsamplin é€šè¿‡æ»¤æ³¢å’Œä¸‹é‡‡æ ·æ¥è®¡ç®—è¾“å…¥å›¾åƒçš„è¿‘ä¼¼å€¼\n",
    "    2.Upsample the outputof step 1 and filter the result (interpolation)å¯¹æ­¥éª¤ 1 çš„è¾“å‡ºè¿›è¡Œä¸Šé‡‡æ ·å¹¶æ»¤æ³¢ï¼ˆæ’å€¼ï¼‰\n",
    "    3. Compute the difference between the prediction of step 2 and the input to step è®¡ç®—æ­¥éª¤ 2 çš„é¢„æµ‹å€¼ä¸æ­¥éª¤ 1 çš„è¾“å…¥å€¼ä¹‹é—´çš„å·®å¼‚\n",
    "    To reconstruct the image:\n",
    "        1.å¯¹æœ€ä½åˆ†è¾¨ç‡çš„è¿‘ä¼¼å›¾åƒè¿›è¡ŒUpsampleä¸Šé‡‡æ ·å’Œfilteræ»¤æ³¢\n",
    "        2.Add the one-level higher prediction residual æ·»åŠ ä¸€å±‚æ›´é«˜çš„é¢„æµ‹æ®‹å·®\n",
    "    Application: Approximation/Residual pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072c10a",
   "metadata": {},
   "source": [
    "# Week3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffedc118",
   "metadata": {},
   "source": [
    "3.Feature Representation\n",
    "\n",
    "3.1.image features\n",
    "vectors that are a compact representation of images\n",
    "why no pixels: change & highly redundant\n",
    "\n",
    "3.1.1.features properties\n",
    "Reproducibility (robustness)\n",
    "Saliency (descriptiveness) ä¸åŒå›¾åƒä¸­çš„ç›¸ä¼¼çªå‡ºç‚¹åº”å…·æœ‰ç›¸ä¼¼çš„ç‰¹å¾\n",
    "Saliency (descriptiveness)\n",
    "\n",
    "3.1.2.color features\n",
    "Invariant to image scaling, translation and rotation\n",
    "\n",
    "3.1.2.1.color histogram: Represent the global distribution of pixel colours in an image\n",
    "\n",
    "3.1.2.2.Color moments: MomentsçŸ©é˜µ based representation of colour distributions\n",
    "    ä¸€é˜¶å‡å€¼ï¼ŒäºŒé˜¶æ ‡å‡†å·®ï¼Œä¸‰é˜¶ååº¦\n",
    "    only 9 elements (for RGB images)\n",
    "    Lower representation capability\n",
    "\n",
    "3.1.3.Texture features\n",
    "a powerful discriminating feature for identifying visual patterns\n",
    "\n",
    "3.1.3.1.Haralick Features(è¦è€ƒæ„å»ºï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼)\n",
    "gives an array of statistical descriptors of image patterns to capture the spatial relationship between neighbouring pixels\n",
    "\n",
    "3.1.3.1.1.Step 1: Construct the gray-level co-occurrence matrix (GLCM)\n",
    "    representing the frequency of pixel intensity pairs occurring at a specific offset and direction\n",
    "    1.Given distance ğ‘‘ and orientation angle ğ‘\n",
    "    2.Compute co-occurrence count (or probability) ğ‘(%,') ğ‘–), ğ‘–\" of going from gray level ğ‘–) to ğ‘–\" at ğ‘‘ and ğ‘\n",
    "    3.Construct matrix ğ(%,') ğ‘–), ğ‘–\" with elements ğ‘–), ğ‘–\" being ğ‘(%,') ğ‘–), ğ‘–\"\n",
    "    4.If an image has ğ¿ distinct gray levels, the matrix size is ğ¿Ã—ğ¿\n",
    "    tips:1.For computational efficiency ğ¿ can be reduced by binningåˆ†ç®±\n",
    "         2.Different co-occurrence matrices can be constructed by using various combinations of distance ğ‘‘ and angular orientation ğ‘\n",
    "         3.On their own these co-occurrence matrices do not provide any measure of texture that can be easily used as texture descriptors\n",
    "         4.The information in the co-occurrence matrices needs to be further extracted as a set of feature values such as the Haralick descriptors\n",
    "\n",
    "3.1.3.1.2.Step 2: Compute the Haralick feature descriptors from the GLCM\n",
    "    that summarises texture information (how pixel intensities are spatially related)\n",
    "\n",
    "3.1.3.1.3.Application: in medical imaging studies due to their simplicity and interpretability\n",
    "\n",
    "3.1.3.2.Local binary patternsæœ¬åœ°äºŒè¿›åˆ¶æ¨¡å¼\n",
    "    Describe the spatial structure of local image texture\n",
    "    1.Divide the image into cells of ğ‘Ã—ğ‘ pixels (for example ğ‘ = 16 or 32)\n",
    "    2.Compare each pixel in a given cell to each of its 8 neighbouring pixels\n",
    "    3.If the centre pixel value is greater than the neighbour value, write 0, otherwise write 1\n",
    "    4.This gives an 8-digit binary pattern per pixel, representing a value in the range 0â€¦255\n",
    "    5.Count the number of times each 8-digit binary number occurs in the cell\n",
    "    6.This gives a 256-bin histogram (also known as the LBP feature vector)\n",
    "    7.Combine the histograms of all cells of the given image\n",
    "    8.This gives the image-level LBP feature descriptor\n",
    "\n",
    "    LBP can be multiresolution and rotation-invariant\n",
    "    Multiresolutionå¤šåˆ†è¾¨ç‡: vary the distance between the centre pixel and neighbouring pixels and vary the number of neighbouring pixels\n",
    "    Rotation-invariantæ—‹è½¬ä¸å˜: vary the way of constructing the 8-digit binary number by performing bitwise shift to derive the smallest number\n",
    "        not all patterns have 8 shifted variant\n",
    "    Application: Texture classification\n",
    "\n",
    "3.1.3.3.Scale-Invariant Feature Transformå°ºåº¦ä¸å˜ç‰¹å¾å˜æ¢\n",
    "    describes texture in a localised region around a keypoint\n",
    "    descriptor is invariant to various transformations\n",
    "\n",
    "3.1.3.3.1.Step 1:SIFT Extrema Detectioæå€¼æ£€æµ‹: Detect maxima and minima in the scale space of the image\n",
    "3.1.3.3.2.Step 2:SIFT Keypoint Localizationå…³é”®ç‚¹å®šä½: Improve and reduce the set of found keypoints\n",
    "    Use 3D quadratic fitting in scale-space to get subpixel optima\n",
    "    Reject low-contrast and edge points using Hessian analysis\n",
    "3.1.3.3.3.Step 3:SIFT Orientation Assignmentæ–¹å‘åˆ†é…: Estimate keypoint orientation using local gradient vectors\n",
    "    Make an orientation histogram of local gradient vectors\n",
    "    Find the dominant orientation from the main peak of the histogram\n",
    "    Create additional keypoint for second highest peak if >80%\n",
    "3.1.3.3.4.Step 4:SIFT Keypoint Descriptorå…³é”®ç‚¹æè¿°ç¬¦:Represent each keypoint by a 128D feature vector\n",
    "    4 x 4 array of gradient histogram weighted by magnitude\n",
    "    8 bins in gradient orientation histogram\n",
    "    Total 8 x 4 x 4 array = 128 dimensions\n",
    "    Each keypoint represented by a 128D feature vector\n",
    "    Descriptor matchingæè¿°ç¬¦åŒ¹é…: Using the nearest neighbour distance ratio (NNDR)\n",
    "3.1.3.3.5.Application: Matching two partially overlapping images\n",
    "\n",
    "\n",
    "3.1.3.4.spatial transformationç©ºé—´å˜æ¢\n",
    "    Rigid transformations: Translation,Rotation\n",
    "    Nonrigid transformations: Scaling,Affine,Perspective\n",
    "\n",
    "3.1.3.5.Fitting and alignmentå®‰è£…å’Œå¯¹é½\n",
    "    Least-squares (LS) fitting of corresponding keypoints ğ±-, ğ±-\n",
    "3.1.3.5.2.RANdom SAmple Consensus éšæœºæŠ½æ ·ä¸€è‡´æ€§(RANSAC) fitting\n",
    "    1. Sample (randomly) the number of points required to fit the model\n",
    "    2. Solve for the model parametersæ¨¡å‹å‚æ•° using the samples\n",
    "    3. Scoreå¾—åˆ† by the fraction of inliers within a preset threshold of the model\n",
    "    Repeat 1-3 until the best model is found with high confidence\n",
    "\n",
    "3.1.3.6.Feature encoding ç‰¹å¾ç¼–ç \n",
    "    Global encoding of local SIFT featuresæœ¬åœ° SIFT ç‰¹å¾çš„å…¨å±€ç¼–ç : Combine local SIFT keypoint descriptors of an image into one global vector\n",
    "3.1.3.6.2.Most popular method: Bag-of-Words (BoW)\n",
    "    Variable number of local image features\n",
    "    Encoded into a fixed-dimensional histogram\n",
    "    1.Step 1\n",
    "        Extract local SIFT keypoint descriptors from training images\n",
    "        Create the â€œvocabularyâ€ from the set of SIFT keypoint descriptors\n",
    "        This vocabulary represents the categories of local descriptors\n",
    "        Main technique used to create the vocabulary is k-means clustering\n",
    "        One of the simplest and most popular unsupervised learning approaches\n",
    "        Performs automatic clustering (partitioning) of the training data into k categories\n",
    "    2.k-means clustering: kå‡å€¼èšç±»\n",
    "        Initialize: k cluster centres (typically randomly)\n",
    "        Iterate: 1. Assign data (feature vectors) to the closest cluster (Euclidean distance)\n",
    "                 2. Update cluster centres as the mean of the data samples in each cluster\n",
    "        Terminate: When converged or the number of iterations reaches the maximum\n",
    "    2.Step 2\n",
    "        Cluster centres are the â€œvisual wordsâ€ in this â€œvocabularyâ€ used to represent an image\n",
    "        Each local feature descriptor is assigned to one visual word with the smallest distance\n",
    "        Compute the number of local image feature descriptors assigned to each visual word\n",
    "        Concatenate the numbers into a vector which is the â€œBoWâ€ representation of the image\n",
    "3.1.3.6.3: Application:SIFT-based texture classification\n",
    "    Build vocabulary Train classifier Classify image\n",
    "\n",
    "3.1.4: Shape featureså½¢çŠ¶ç‰¹å¾\n",
    "    Shape is an essential characteristic of material objects\n",
    "    Shape features are typically extracted after image segmentation\n",
    "    They can be used to identify and classify objects\n",
    "    Example: object recognitionå¯¹è±¡è¯†åˆ«\n",
    "3.1.4.1: Challenges in defining shape features\n",
    "    Invariant to rigid transformations\n",
    "    Tolerant to non-rigid deformations\n",
    "    Unknown correspondence\n",
    "3.1.4.2: Basic shape features\n",
    "    Convexity versus concavity of an object\n",
    "    Convex hull of an object\n",
    "    Convex deficiency of an object\n",
    "    Simple geometrical shape descriptors:\n",
    "        Compactness inversely related Circularity\n",
    "        Elongation & Eccentricity\n",
    "3.1.4.3: Boundary descriptorsè¾¹ç•Œæè¿°ç¬¦\n",
    "    Chain code descriptoré“¾ç æè¿°ç¬¦\n",
    "        Represents object shape by the relative positions of consecutive boundary points\n",
    "        Consists of a list of directions from a starting point\n",
    "        Provides a compact boundary representation\n",
    "    Local curvature descriptorå±€éƒ¨æ›²ç‡æè¿°ç¬¦\n",
    "        The curvature of an object is a local shape attribute\n",
    "        Convex (versus concave) parts have positive (versus negative) curvature\n",
    "    Two interpretations of local curvatureå±€éƒ¨æ›²ç‡çš„ä¸¤ç§è§£é‡Š\n",
    "        Geometrical & Physical\n",
    "    Global curvature descriptorså…¨å±€æ›²ç‡æè¿°ç¬¦\n",
    "        Total bending energyæ€»å¼¯æ›²èƒ½é‡ & Total absolute curvatureæ€»ç»å¯¹æ›²ç‡\n",
    "    Radial distance descriptor å¾„å‘è·ç¦»æè¿°ç¬¦\n",
    "3.1.4.4:Application: Combining feature descriptors to classify objects ç»„åˆç‰¹å¾æè¿°ç¬¦ä»¥å¯¹å¯¹è±¡è¿›è¡Œåˆ†ç±»\n",
    "\n",
    "3.1.4.5.Shape contextå½¢çŠ¶ä¸Šä¸‹æ–‡\n",
    "    Shape context is a point-wise local feature descriptoré€ç‚¹å±€éƒ¨ç‰¹å¾æè¿°ç¬¦\n",
    "    Pick ğ‘› points ğ‘\" on the contour of a shape\n",
    "    For each point, create a radial coordinate system centred at this point and compute a histogram â„\" based on the relative coordinates of the other ğ‘› âˆ’ 1 points\n",
    "    This is the shape context of ğ‘\"\n",
    "    Application: Shape matching\n",
    "        Step 1: Sample a list of points on shape edges For example from Canny edge detector\n",
    "        Step 2: Compute the shape context for each point\n",
    "        Step 3: Compute the cost matrix between two shapes ğ‘ƒ and ğ‘„\n",
    "        Step 4: Find the one-to-one matching minimising the total cost between point pairs\n",
    "        Step 5: Transform one shape to the other based on the one-to-one point matching\n",
    "        Step 6: Compute the shape distance\n",
    "\n",
    "3.1.4.6.Histogram of oriented gradientså®šå‘æ¢¯åº¦ç›´æ–¹å›¾\n",
    "    Describes the distributions of gradient orientations in localized areas æè¿°å±€éƒ¨åŒºåŸŸä¸­æ¢¯åº¦æ–¹å‘çš„åˆ†å¸ƒ\n",
    "    Does not require initial segmentation ä¸éœ€è¦åˆå§‹åˆ†å‰²\n",
    "    Step 1: Calculate the gradient vectoræ¢¯åº¦çŸ¢é‡ at each pixel\n",
    "        Gradient magnitude & Gradient orientation\n",
    "    Step 2: Construct the gradient histogram of all pixels in a cell\n",
    "        Divide orientations into ğ‘ bins (typically ğ‘ = 9 bins evenly splitting 180 degrees)\n",
    "        Assign the gradient magnitude of each pixel to the bin corresponding to its orientation\n",
    "    Step 3: Generate detection-window level HOG descriptorç”Ÿæˆæ£€æµ‹çª—å£çº§åˆ«çš„ HOG æè¿°ç¬¦\n",
    "        Window Block Cell\n",
    "    Detection via sliding window on the image é€šè¿‡å›¾åƒä¸Šçš„æ»‘åŠ¨çª—å£è¿›è¡Œæ£€æµ‹\n",
    "        Compute the HOG descriptor for many example windows from a training dataset\n",
    "        Manually label each example window as either â€œpersonâ€ or â€œbackgroundâ€\n",
    "        Train a classifier (such as SVM) from these example windows and labels\n",
    "        For each new (test) image predict the label of each window using this classifier\n",
    "    Application: Detecting humans in images\n",
    "                 Detecting and tracking humans in videos\n",
    "                 Fine-grained detection using deformable parts model ä½¿ç”¨å¯å˜å½¢é›¶ä»¶æ¨¡å‹è¿›è¡Œç»†ç²’åº¦æ£€æµ‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2e464",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16362c5b",
   "metadata": {},
   "source": [
    "4.1Pattern recognition\n",
    "4.1.1.Supervised learning\n",
    "    Learning patterns in a set of data with available labels (ground truth)\n",
    "    4.1.1.1.Concepts\n",
    "        Objects are (identifiable) physical entities of which images are taken\n",
    "        Regions (ideally) correspond to objects after image segmentation\n",
    "        Classes are disjoint subsets of objects sharing common features\n",
    "        Labels are associated with objects and indicate to which class they belong\n",
    "        Classification is the process of assigning labels to objects based on features\n",
    "        Classifiers are algorithms/methods performing the classification task\n",
    "        Patterns are regularities in object features and are used by classifiers\n",
    "        Pattern Recognition Systems\n",
    "            Basic stages involved in the design of a classification system\n",
    "        Pre-processing aims to enhance images for further processing\n",
    "        Feature extraction reduces the data by measuring certain properties\n",
    "        Feature descriptors represent scalar properties of objects\n",
    "        Feature vectors capture all the properties measured from the data\n",
    "        Feature selection aims to keep only the most descriptive features\n",
    "        Models are (mathematical or statistical) descriptions of classes\n",
    "        Training samples are objects with known labels used to build models\n",
    "        Cost is the consequence of making an incorrect decision/assignment\n",
    "        Decision boundary is the demarcation between regions in feature space\n",
    "    4.1.1.2.Feature Vector Representation\n",
    "        Features represent knowledge about the object and go by other names such as predictors, descriptors,covariates, independent variablesâ€¦\n",
    "    4.1.1.3.Feature Extraction\n",
    "        Characterise objects by measurements that are\n",
    "            Similar for objects in the same class/category\n",
    "            Different for objects in different classes\n",
    "        Use distinguishing features\n",
    "            Invariant to object position (translation)\n",
    "            Invariant to object orientation (rotation)\n",
    "            Invariant to â€¦ (depends on the application)\n",
    "            Good examples are shape, colour, texture\n",
    "        Design of features often based on prior experience or intuition\n",
    "        Select features that are robust to\n",
    "            Rigid transformations (translation and rotation)\n",
    "            Occlusions and other 3D-to-2D projective distortions\n",
    "            Non-rigid/articulated object deformations (e.g. fingers around a cup)\n",
    "            Variations in illumination and shadows\n",
    "        Feature selection is problem- and domain-dependent and requires domain knowledge\n",
    "        Classification techniques can help to make feature values less noise sensitive and to select valuable features out of a larger set\n",
    "    4.1.1.4.Pattern Recognition Models\n",
    "        Generative models\n",
    "            Model the â€œmechanismâ€ by which the data was generated\n",
    "            Represent each class by a probabilistic â€œmodelâ€ ğ‘ ğ‘¥|ğ‘¦ and ğ‘ ğ‘¦\n",
    "            Obtain the joint probability of the data as ğ‘ ğ‘¥, ğ‘¦ = ğ‘ ğ‘¥|ğ‘¦ ğ‘ ğ‘¦\n",
    "            Find the decision boundary implicitly via the most likely ğ‘ ğ‘¦|ğ‘¥\n",
    "            Applicable to unsupervised learning tasks (unlabelled data)\n",
    "        Discriminative models\n",
    "            Focus on explicit modelling of the decision boundary\n",
    "            Applicable to supervised learning tasks (labelled data)\n",
    "    4.1.1.5.Classification\n",
    "        Classifier performs object recognition by assigning a class label to an object, using the object description in the form of features\n",
    "        Perfect classification is often impossible, instead determine the probability for each possible class\n",
    "        Difficulties are caused by variability in feature values for objects in the same class versus objects in different classes\n",
    "        Variability may arise due to complexity but also due to noise\n",
    "        Noisy features and missing features are major issues\n",
    "        Not always possible to determine values of all features for an object\n",
    "    4.1.1.5.1.Binary Classification\n",
    "    4.1.1.5.2.Nearest Class Mean Classifier\n",
    "        Pros\n",
    "            Simple\n",
    "            Fast\n",
    "            Works well when classes are compact and far from each other\n",
    "        Cons\n",
    "            Poor results for complex classes (multimodal, non-spherical)\n",
    "            Cannot handle outliers and noisy data well\n",
    "            Cannot handle missing data\n",
    "    4.1.1.5.3.K-Nearest Neighbours Classifier\n",
    "        Pros\n",
    "            Very simple and intuitive\n",
    "            Easy to implement\n",
    "            No a priori assumptions\n",
    "            No training step\n",
    "            Decision surfaces are non-linear\n",
    "        Cons\n",
    "            Slow algorithm for big data sets\n",
    "            Needs homogeneous (similar nature) feature types and scales\n",
    "            Does not perform well when the number of variables grows (curse of dimensionality)\n",
    "            Finding the optimal K (number of neighbours) to use can be challenging\n",
    "        Applications\n",
    "            Automated MS-lesion segmentation by KNN\n",
    "    4.1.1.5.3.Bayesian Decision Theory\n",
    "        Risk:\n",
    "            If we only care about the classification accuracy (the prices of all types of fish are the same), then we can make the decision by maximizing the posterior probability\n",
    "            If the prices are not the same, we minimize the loss\n",
    "                Cost of an action ğ›¼! based on our decision: ğœ† ğ›¼! ğ‘!\n",
    "                The expected loss associated with action ğ›¼! is:\n",
    "                    ğ‘… ğ›¼! ğ‘¥ = âˆ‘8 ğœ† ğ›¼! ğ‘8 ğ‘(ğ‘8|ğ‘¥)\n",
    "                ğ‘… ğ›¼! ğ‘¥ is also called conditional risk\n",
    "                An optimal Bayes decision strategy is to minimize the conditional risk\n",
    "        Pros\n",
    "            Simple and efficient\n",
    "            Considers uncertainties\n",
    "            Permits combining new information with current knowledge\n",
    "        Cons\n",
    "            Struggles with complex data relationships\n",
    "            Choice of priors can be subjective\n",
    "    4.1.1.6.Decision Trees\n",
    "        Most pattern recognition methods address problems where feature vectors are real-valued and there exists some notion of a metric\n",
    "        Some classification problems involve nominal data, with discrete descriptors and without a natural notion of similarity or ordering\n",
    "            Example: {high, medium, low}, {red, green, blue}\n",
    "            Nominal data are also called as categorical data\n",
    "        Nominal data can be classified using rule-based method\n",
    "        Continuous values can also be handled with rule-based method\n",
    "        Pros\n",
    "            Easy to interpret\n",
    "            Can handle both numerical and categorical data\n",
    "            Robust to outliers and missing values\n",
    "            Gives information on importance of features (feature selection)\n",
    "        Cons\n",
    "            Tends to overfit\n",
    "            Only axis-aligned splits\n",
    "            Greedy algorithm (may not find the best tree)\n",
    "\n",
    "\n",
    "\n",
    "    4.1.8.Support Vector Machines (SVMs)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "4.1.2.Unsupervised learning\n",
    "Finding patterns in a set of data without any labels available\n",
    "4.1.3.Semi-supervised learning\n",
    "Using a combination of labelled and unlabelled data to learn patterns\n",
    "4.1.4.Weakly supervised learning\n",
    "Using noisy / limited / imprecise supervision signals in learning of patterns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
